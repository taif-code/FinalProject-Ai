import os
import speech_recognition as sr
from PIL import Image
import torch
import json
import faiss
from langdetect import detect

from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from transformers import CLIPModel, CLIPProcessor

# âœ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­
os.environ["OPENAI_API_KEY"] = "your_openai_key_here"
os.environ["SERPAPI_API_KEY"] = "your_serpapi_key_here"


# âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
llm = ChatOpenAI(model="gpt-4o", temperature=0, api_key=os.getenv("OPENAI_API_KEY"))
embeddings = OpenAIEmbeddings()
vectordb = FAISS.load_local("faiss_landmarks", embeddings, allow_dangerous_deserialization=True)
retriever = vectordb.as_retriever(search_kwargs={"k": 3})

# âœ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ CLIP
device = "cuda" if torch.cuda.is_available() else "cpu"
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
index = faiss.read_index("/Users/taifaladwani/Downloads/SmartTourGuide/data/data/embeddings/index.faiss")
with open("/Users/taifaladwani/Downloads/SmartTourGuide/landmarks_list2.json") as f:
    labels = json.load(f)

# âœ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ©
def get_prompt_template_by_lang(lang="ar"):
    if lang == "ar":
        return PromptTemplate(
            input_variables=["context", "question"],
            template="""Ø£Ù†Øª Ù…Ø±Ø´Ø¯ Ø³ÙŠØ§Ø­ÙŠ Ø°ÙƒÙŠ. 
Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø¯Ù‚Ø© ÙˆØ¨Ø£Ø³Ù„ÙˆØ¨ Ù…Ø®ØªØµØ±:

ğŸ“š Ø§Ù„Ø³ÙŠØ§Ù‚:  
{context}

â“ Ø§Ù„Ø³Ø¤Ø§Ù„:
{question}

ğŸ§  Ø§Ù„Ø¬ÙˆØ§Ø¨:""")
    else:
        return PromptTemplate(
            input_variables=["context", "question"],
            template="""You are a smart tour guide. Use the following context to answer the user's question accurately:

ğŸ“š Context:  
{context}

â“ Question:
{question}

ğŸ§  Answer:""")

# âœ… ØªØ­Ù„ÙŠÙ„ Ù†Øµ Ø¹Ø¨Ø± RAG
def rag_tool_func(query: str):
    lang = detect(query)
    prompt = get_prompt_template_by_lang(lang)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )

    rag_result = qa_chain({"query": query})
    answer = rag_result.get("result", "")

    if not answer or len(answer) < 30:
        return "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©.", lang, "Wikipedia / Google"

    return answer, lang, "Knowledge Base"

# âœ… ØªØ­ÙˆÙŠÙ„ ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ â†’ Ø«Ù… ØªØ­Ù„ÙŠÙ„
def audio_tool_func(audio_path: str, lang="ar"):
    r = sr.Recognizer()
    with sr.AudioFile(audio_path) as source:
        audio = r.record(source)

    try:
        text = r.recognize_google(audio, language=lang)
        print("ğŸ¤ ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ:", text)
        return rag_tool_func(text)
    except sr.UnknownValueError:
        return "âŒ Ù„Ù… ÙŠØªÙ… ÙÙ‡Ù… Ø§Ù„ØµÙˆØª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.", lang, "SpeechRecognition"
    except Exception as e:
        return f"âŒ Ø®Ø·Ø£: {e}", lang, "Error"

# âœ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ù…Ø¹Ù„Ù… Ù…Ù† ØµÙˆØ±Ø© â†’ Ø«Ù… ØªØ­Ù„ÙŠÙ„
def recognize_image(image_path: str) -> str:
    image = Image.open(image_path).convert("RGB")
    inputs = clip_processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        img_emb = clip_model.get_image_features(**inputs).cpu().numpy().astype('float32')
    _, indices = index.search(img_emb, k=1)
    idx = indices[0][0]
    return labels[idx] if idx < len(labels) else "Unknown Landmark"

def image_tool_func(image_path: str):
    landmark = recognize_image(image_path)
    print("ğŸ–¼ï¸ ØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„Ù…:", landmark)

    if landmark == "Unknown Landmark":
        return "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„Ù….", "ar", "CLIP"
    
    return rag_tool_func(landmark)
